{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFbGR05cXFPr"
      },
      "source": [
        "# WARNING\n",
        "\n",
        "This scraper gets detected by the server and you will get bolcked.\n",
        "\n",
        "DO NOT try on public networks to avoid blocking your IP.\n",
        "\n",
        "Use on Google Colab is recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhpy3vDWY7U_"
      },
      "source": [
        "# Limitations\n",
        "\n",
        "1.   The server detects activities. Continuous scraping or scrapning data for an author with more than a couple of hundred publications throws erros. As a temporary solution, the partial results are pickled and is later loaded to try and scrape the remaining data. The following solutions did not work:\n",
        "\n",
        "*   Using sleep between requests.\n",
        "*   Using proxies (due to connection challenges).\n",
        "\n",
        "\n",
        "2.   The returned HTML sometimes does not contain some of the details (marked with a #TODO comment). As of October, 2022, these details include the link to the publication, the paper description, and the link to the author's photo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AVulhf6TTqN"
      },
      "source": [
        "# Google Scholar Scraper\n",
        "\n",
        "1.   Set the AUTHORID variable in the following.\n",
        "2.   If on Google Colab, mount your Google Drive.\n",
        "3.   Define your DATA_PATH variable.\n",
        "4.   Create your author object using:\n",
        "```\n",
        "author_obj = create_author(AUTHORID)\n",
        "```\n",
        "5.   Scrape the data about the author using:\n",
        "```\n",
        "author_obj.scrape()\n",
        "```\n",
        "6.   To see if all publications details are retrieved, check:\n",
        "```\n",
        "author_obj.all_publications_extracted\n",
        "```\n",
        "7.   If the previous step gives you False, run:\n",
        "```**bold text**\n",
        "author_obj.scrape()\n",
        "```\n",
        "\n",
        "The data will be saved as both pickled files and json files in your DATA_PATH. If you are trying to re-scrape data on an author from scratch, destroy the pickled file named AUTHORID.pkl before creating the author_object again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6FlZ8EUk3G"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO-BLXsnTOQ5",
        "outputId": "4d441106-bb13-45d8-ed40-42dac54e24d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "AUTHORID = 'TkVleDIAAAAJ'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/PhD/CS848/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2hQDE7KUher"
      },
      "source": [
        "## Scrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEKktAX5dndU",
        "outputId": "eb412e93-3701-493c-9536-a5fabc8f02e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "import json, pickle, requests, bs4, re, time, sys\n",
        "\n",
        "sys.setrecursionlimit(1000000)\n",
        "\n",
        "class Publication(object):\n",
        "  def __init__(self, title, year, cited_by, url, cookies,\n",
        "               link=None,\n",
        "               authors=None,\n",
        "               description=None,\n",
        "               citation_histogram=None,\n",
        "               detail_extracted=False):\n",
        "    self.url = f'https://scholar.google.com/citations?{url}'\n",
        "    self.cookies = cookies\n",
        "    self.headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',\n",
        "                    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
        "    self.soup = ''\n",
        "    self.title = title\n",
        "    self.year = year\n",
        "    self.cited_by = cited_by\n",
        "    self.link = link\n",
        "    self.authors = authors\n",
        "    self.description = description\n",
        "    self.citation_histogram = citation_histogram\n",
        "    self.detail_extracted = detail_extracted\n",
        "    \n",
        "  def make_detail_request(self):\n",
        "    response = requests.request(\"GET\",self.url,headers=self.headers,cookies=self.cookies)\n",
        "    if response.status_code == 200:\n",
        "      self.detail_extracted = True\n",
        "    return response\n",
        "  \n",
        "  # TODO: For some reason, stopped working.\n",
        "  # The response html is different and does not contain this information)\n",
        "  def get_link_to_publication(self):\n",
        "    title = self.soup.find('div', {'id': 'gsc_oci_title'})\n",
        "    if title:\n",
        "      link = title.find('a', {'class': 'gsc_oci_title_link'})\n",
        "      if link:\n",
        "        self.link = link.get('href')\n",
        "  \n",
        "  def get_authors(self):\n",
        "    authors = self.soup.find('div', text = re.compile('Authors'), attrs = {'class' : 'gsc_oci_field'})\n",
        "    if authors:\n",
        "      self.authors = authors.parent.find('div', {'class': 'gsc_oci_value'}).get_text().split(', ')\n",
        "    else:\n",
        "      inventors = self.soup.find('div', text = re.compile('Inventors'), attrs = {'class' : 'gsc_oci_field'})\n",
        "      if inventors:\n",
        "        self.authors = inventors.parent.find('div', {'class': 'gsc_oci_value'}).get_text().split(', ')\n",
        "\n",
        "  # TODO: For some reason, stopped working.\n",
        "  # The response html is different and does not contain this information)\n",
        "  def get_description(self):\n",
        "    description = self.soup.find('div', {'id': 'gsc_oci_descr'})\n",
        "    if description:\n",
        "      self.description = description.get_text()\n",
        "  \n",
        "  def get_citation_histogram(self):\n",
        "    citation_hist = self.soup.find('div', {'id': 'gsc_oci_graph_bars'})\n",
        "    if citation_hist:\n",
        "      citation_hist_time = list(map(lambda x: int(x.get_text()),citation_hist.find_all('span', {'class': 'gsc_oci_g_t'})))\n",
        "      citation_hist_cites = list(map(lambda x: int(x.get_text()),citation_hist.find_all('span', {'class': 'gsc_oci_g_al'})))\n",
        "      self.citation_histogram = list(zip(citation_hist_time,citation_hist_cites))\n",
        "  \n",
        "  def scrape(self):\n",
        "    self.soup = bs4.BeautifulSoup(self.make_detail_request().content, 'lxml')\n",
        "    if self.detail_extracted:\n",
        "      self.get_link_to_publication()\n",
        "      self.get_authors()\n",
        "      self.get_description()\n",
        "      self.get_citation_histogram()\n",
        "    return self.detail_extracted\n",
        "  \n",
        "  def export_json(self):\n",
        "    return {\n",
        "        'title': self.title,\n",
        "        'link': self.link,\n",
        "        'year': self.year,\n",
        "        'cited_by': self.cited_by,\n",
        "        'authors': self.authors,\n",
        "        'description': self.description,\n",
        "        'citation_histogram': self.citation_histogram,\n",
        "        'detail_extracted': self.detail_extracted\n",
        "    }\n",
        "\n",
        "class Author(object):\n",
        "  data_path = DATA_PATH\n",
        "\n",
        "  def __init__(self, authorID,\n",
        "               name=None, \n",
        "               image_link=None,\n",
        "               interests=None,\n",
        "               citations=None,\n",
        "               hindex=None,\n",
        "               i10index=None,\n",
        "               citation_histogram=None,\n",
        "               coauthors=None, \n",
        "               publications=None,\n",
        "               all_publications_retrieved=False,\n",
        "               all_publications_extracted=False):\n",
        "    \n",
        "    self.authorID = authorID\n",
        "    self.cstart = 0\n",
        "    self.pagesize = 100 # Max page size in scholar\n",
        "    self.cookies = ''\n",
        "    self.headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',\n",
        "                    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
        "    self.soup = None\n",
        "    self.name = name\n",
        "    self.image_link = image_link\n",
        "    self.interests = interests\n",
        "    self.citations = citations\n",
        "    self.hindex = hindex\n",
        "    self.i10index = i10index\n",
        "    self.citation_histogram = citation_histogram\n",
        "    self.coauthors = coauthors\n",
        "    self.publications = publications\n",
        "    self.all_publications_retrieved = all_publications_retrieved\n",
        "    self.all_publications_extracted = all_publications_extracted\n",
        "\n",
        "  def set_url(self):\n",
        "    return f\"https://scholar.google.com/citations?hl=en&user={self.authorID}&cstart={self.cstart}&pagesize={self.pagesize}\"\n",
        "\n",
        "  def make_profile_request(self):\n",
        "    url = self.set_url()\n",
        "    response = requests.request(\"GET\",url,headers=self.headers,cookies=self.cookies)\n",
        "    if response.status_code == 429:\n",
        "      raise Exception(\"The server responded with Error 429. We have been detected. Wait before trying again.\")\n",
        "    self.cookies = response.cookies\n",
        "    return response\n",
        "\n",
        "  def make_coauthor_request(self):\n",
        "    url = self.set_url()+'&view_op=list_colleagues'\n",
        "    response = requests.request(\"GET\",url,headers=self.headers)\n",
        "    if response.status_code == 429:\n",
        "      raise Exception(\"The server responded with Error 429. We have been detected. Wait before trying again.\")\n",
        "    return response\n",
        "    \n",
        "  def get_full_name(self):\n",
        "    name = self.soup.find('div', {'id': 'gsc_prf_in'})\n",
        "    if name:\n",
        "      self.name = name.get_text()\n",
        "  \n",
        "  # TODO: For some reason, stopped working.\n",
        "  # The response html is different and does not contain this information)\n",
        "  def get_image_link(self):\n",
        "    image = self.soup.find('div', {'img': 'gsc_prf_pup-img'})\n",
        "    if image:\n",
        "      self.image_link = image.get('src')\n",
        "  \n",
        "  def get_interests(self):\n",
        "    self.interests = list(map(lambda x: x.get_text(), self.soup.find_all('a', {'class': 'gsc_prf_inta'})))\n",
        "\n",
        "  def get_citations_count(self, citation_info):\n",
        "    citation = citation_info.find('a', text = re.compile('Citations'), attrs = {'class' : 'gsc_rsb_f'})\n",
        "    if citation:\n",
        "      citation_value = citation.parent.parent.find_all('td', {'class': 'gsc_rsb_std'})\n",
        "      if len(citation_value)>0:\n",
        "        self.citations = int(citation_value[0].get_text())\n",
        "  \n",
        "  def get_hindex(self, citation_info):\n",
        "    hindex = citation_info.find('a', text = re.compile('h-index'), attrs = {'class' : 'gsc_rsb_f'})\n",
        "    if hindex:\n",
        "      hindex_value = hindex.parent.parent.find_all('td', {'class': 'gsc_rsb_std'})\n",
        "      if len(hindex_value)>0:\n",
        "        self.hindex = int(hindex_value[0].get_text())\n",
        "  \n",
        "  def get_i10index(self, citation_info):\n",
        "    i10index = citation_info.find('a', text = re.compile('i10-index'), attrs = {'class' : 'gsc_rsb_f'})\n",
        "    if i10index:\n",
        "      i10index_value = i10index.parent.parent.find_all('td', {'class': 'gsc_rsb_std'})\n",
        "      if len(i10index_value)>0:\n",
        "        self.i10index = int(i10index_value[0].get_text())\n",
        "    \n",
        "  def get_citation_metrics(self):\n",
        "    citation_info = self.soup.find('div', {'id': 'gsc_rsb_cit'})\n",
        "    if citation_info:\n",
        "      self.get_citations_count(citation_info)\n",
        "      self.get_hindex(citation_info)\n",
        "      self.get_i10index(citation_info)\n",
        "  \n",
        "  def get_citation_histogram(self):\n",
        "    citation_hist = self.soup.find_all('div', {'class': 'gsc_md_hist_w'})\n",
        "    if citation_hist:\n",
        "      citation_hist = citation_hist[0]\n",
        "      citation_hist_time = list(map(lambda x: x.get_text(),citation_hist.find_all('span', {'class': 'gsc_g_t'})))\n",
        "      citation_hist_cites = list(map(lambda x: x.get_text(),citation_hist.find_all('a', {'class': 'gsc_g_a'})))\n",
        "      self.citation_histogram = list(zip(citation_hist_time,citation_hist_cites))\n",
        "\n",
        "  def get_coauthors(self):\n",
        "    coauthor_list = self.soup.find('div', {'id': 'gsc_rsb_co'})\n",
        "    if coauthor_list:\n",
        "      if coauthor_list.find('button'): # too many coauthors requires a request\n",
        "        coauthor_list = bs4.BeautifulSoup(self.make_coauthor_request().content, 'html.parser').find('div', {'id': 'gsc_codb_content'})\n",
        "        coauthor_list = coauthor_list.find_all('div', {'class': 'gsc_ucoar'})\n",
        "        coauthor_ids = list(map(lambda x: x.get('id').split('-')[-1], coauthor_list))\n",
        "        coauthor_names = list(map(lambda x: x.find('img').get('alt'), coauthor_list))\n",
        "        self.coauthors = list(zip(coauthor_ids,coauthor_names))\n",
        "      else:\n",
        "        coauthor_list = coauthor_list.find_all('img')\n",
        "        coauthor_ids = list(map(lambda x: x.get('id').split('-')[1], coauthor_list))\n",
        "        coauthor_names = list(map(lambda x: x.get('alt'), coauthor_list))\n",
        "        self.coauthors = list(zip(coauthor_ids,coauthor_names))\n",
        "  \n",
        "  def extract_compact_publication(self, publication_element):\n",
        "    title = publication_element.find('a',{\"class\": \"gsc_a_at\"}).get_text()\n",
        "    year = publication_element.find('td',{\"class\": \"gsc_a_y\"}).find('span', {\"class\": \"gsc_a_hc\"}).get_text()\n",
        "    if year:\n",
        "      year = int(year)\n",
        "    else:\n",
        "      year = None\n",
        "    url = publication_element.find('a',{\"class\": \"gsc_a_at\"}).get('href').split('?')[-1]\n",
        "    cited_by = publication_element.find('a',{\"class\": \"gsc_a_ac\"}).get_text()\n",
        "    if cited_by:\n",
        "      cited_by = int(cited_by)\n",
        "    else:\n",
        "      cited_by = None\n",
        "    return Publication(title, year, cited_by, url, self.cookies)\n",
        "  \n",
        "  def get_publications_list(self):\n",
        "    publication_list = []\n",
        "    while True:\n",
        "      soup = bs4.BeautifulSoup(self.make_profile_request().content, 'html.parser')\n",
        "      items = soup.find_all('tr', {\"class\": \"gsc_a_tr\"})\n",
        "      if len(items)==1:\n",
        "        if items[0].find('td', {\"class\": \"gsc_a_e\"}):\n",
        "          self.all_publications_retrieved = True\n",
        "          break\n",
        "      publication_list += items\n",
        "      self.cstart += self.pagesize\n",
        "    self.publications = list(map(lambda x: self.extract_compact_publication(x), publication_list))\n",
        "\n",
        "  def get_publications_detail(self):\n",
        "    unscraped_publications = filter(lambda x: not x.detail_extracted, self.publications)\n",
        "    for publication in unscraped_publications:\n",
        "      # time.sleep(5)\n",
        "      successful = publication.scrape()\n",
        "      if not successful:\n",
        "        break\n",
        "    self.set_all_publications_extracted()\n",
        "  \n",
        "  def set_all_publications_extracted(self):\n",
        "    checker = next(filter(lambda x: not x.detail_extracted, self.publications),None)\n",
        "    if checker is None:\n",
        "      self.all_publications_extracted = True\n",
        "    else:\n",
        "      self.all_publications_extracted = False\n",
        "  \n",
        "  def save_data(self):\n",
        "    with open(f'{self.data_path}/{self.authorID}.pkl', 'wb') as f:\n",
        "      pickle.dump(self, f)\n",
        "    if self.all_publications_extracted:\n",
        "      data = self.export_json()\n",
        "      with open(f'{self.data_path}/{self.authorID}.json', 'w') as f:\n",
        "        json.dump(data, f)\n",
        "      print('The data extraction was complete.')\n",
        "      print(f'The extracted information is saved into the \"{self.authorID}.pkl/json\" files in the selected data path.')\n",
        "    else:\n",
        "      print('The publication detail extraction was incomplete.')\n",
        "      print(f'The extracted information is pickled into the \"{self.authorID}.pkl\" file in the selected data path.')\n",
        "      print('We will continue to extract the detail of the remaining publications next time you try.')\n",
        "\n",
        "  def export_json(self):\n",
        "    data = {'authorID': self.authorID,\n",
        "            'name': self.name,\n",
        "            'image_link': self.image_link,\n",
        "            'interests': self.interests,\n",
        "            'citations': self.citations,\n",
        "            'hindex': self.hindex,\n",
        "            'i10index': self.i10index,\n",
        "            'citation_histogram': self.citation_histogram,\n",
        "            'coauthors': self.coauthors,\n",
        "            'publications': [],\n",
        "            'all_publications_retrieved': self.all_publications_retrieved,\n",
        "            'all_publications_extracted': self.all_publications_extracted}\n",
        "    data['publications'] = list(map(lambda x: x.export_json(), self.publications))\n",
        "    return data\n",
        "\n",
        "  def scrape(self):\n",
        "    if not self.all_publications_retrieved:\n",
        "      self.soup = bs4.BeautifulSoup(self.make_profile_request().content, 'html.parser')\n",
        "      self.get_full_name()\n",
        "      self.get_image_link()\n",
        "      self.get_interests()\n",
        "      self.get_citation_metrics()\n",
        "      self.get_citation_histogram()\n",
        "      self.get_coauthors()\n",
        "      self.get_publications_list()\n",
        "    if not self.all_publications_extracted:\n",
        "      self.get_publications_detail()\n",
        "    self.save_data()\n",
        "  \n",
        "def create_author(authorID):\n",
        "  try:\n",
        "    with open(f'{DATA_PATH}/{authorID}.pkl', 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "  except:\n",
        "    return Author(authorID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpAO423PWXKE"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "kyMbLZGUJZCS",
        "outputId": "4c80b994-0baa-4b05-d762-d97aa382fadf"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-edc56b567693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mauthor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUTHORID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauthor_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauthor_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_publications_extracted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-27102f10f455>\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_publications_retrieved\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_profile_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-27102f10f455>\u001b[0m in \u001b[0;36mmake_profile_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The server responded with Error 429. We have been detected. Wait before trying again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The server responded with Error 429. We have been detected. Wait before trying again."
          ]
        }
      ],
      "source": [
        "author_obj = create_author(AUTHORID)\n",
        "author_obj.scrape()\n",
        "author_obj.all_publications_extracted"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
